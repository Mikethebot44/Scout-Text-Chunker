{"version":3,"sources":["../src/chunkers/FixedChunker.ts","../src/utils/tokenizer.ts","../src/chunkers/RecursiveChunker.ts","../src/chunkers/SemanticChunker.ts","../src/utils/cosine.ts","../src/utils/stats.ts","../src/utils/smoothing.ts","../src/algorithms/TextTiling.ts","../src/algorithms/C99.ts","../src/algorithms/BayesSeg.ts","../src/chunkers/HybridChunker.ts","../src/chunkers/TopicChunker.ts","../src/chunkers/SlidingWindowChunker.ts","../src/core/Factory.ts","../src/embedders/OpenAIEmbedder.ts","../src/embedders/HuggingFaceEmbedder.ts","../src/embedders/LocalModelEmbedder.ts"],"sourcesContent":["import { v4 as uuid } from \"uuid\";\r\nimport { Chunk, Chunker, ChunkerOptions } from \"../types\";\r\nimport { tokenize } from \"../utils/tokenizer\";\r\n\r\nexport class FixedChunker implements Chunker {\r\n  private readonly chunkSize: number;\r\n  private readonly chunkOverlap: number;\r\n\r\n  constructor(options: ChunkerOptions) {\r\n    this.chunkSize = options.chunkSize ?? 500;\r\n    this.chunkOverlap = options.chunkOverlap ?? Math.floor(this.chunkSize / 10);\r\n  }\r\n\r\n  async chunk(text: string): Promise<Chunk[]> {\r\n    const tokens = tokenize(text);\r\n    if (!tokens.length) return [];\r\n    const chunks: Chunk[] = [];\r\n    let startIndex = 0;\r\n\r\n    while (startIndex < tokens.length) {\r\n      const endIndex = Math.min(tokens.length, startIndex + this.chunkSize);\r\n      const chunkTokens = tokens.slice(startIndex, endIndex);\r\n      const chunkText = chunkTokens.map((token) => token.value).join(\" \").trim();\r\n      const start = chunkTokens[0].start;\r\n      const end = chunkTokens[chunkTokens.length - 1].end;\r\n      chunks.push({ id: uuid(), text: chunkText, start, end });\r\n      if (endIndex === tokens.length) break;\r\n      startIndex = Math.max(0, endIndex - this.chunkOverlap);\r\n    }\r\n\r\n    return chunks;\r\n  }\r\n}\r\n","import { SentenceBoundary } from \"../types\";\r\n\r\nconst SENTENCE_REGEX = /(?<=[.!?])\\s+/;\r\nconst PARAGRAPH_REGEX = /\\n{2,}/;\r\n\r\nexport interface Token {\r\n  value: string;\r\n  start: number;\r\n  end: number;\r\n}\r\n\r\nexport function tokenize(text: string): Token[] {\r\n  const tokens: Token[] = [];\r\n  let index = 0;\r\n  for (const part of text.split(/(\\s+)/)) {\r\n    if (!part) continue;\r\n    const start = index;\r\n    const end = index + part.length;\r\n    tokens.push({ value: part, start, end });\r\n    index = end;\r\n  }\r\n  return tokens.filter((token) => token.value.trim().length > 0);\r\n}\r\n\r\nexport function countTokens(text: string): number {\r\n  return tokenize(text).length;\r\n}\r\n\r\nexport function splitIntoSentences(text: string): SentenceBoundary[] {\r\n  const sentences: SentenceBoundary[] = [];\r\n  let offset = 0;\r\n  for (const paragraph of text.split(PARAGRAPH_REGEX)) {\r\n    const trimmed = paragraph.trim();\r\n    if (!trimmed) {\r\n      offset += paragraph.length + 2;\r\n      continue;\r\n    }\r\n    const fragments = trimmed.split(SENTENCE_REGEX);\r\n    for (const fragment of fragments) {\r\n      const start = text.indexOf(fragment, offset);\r\n      const end = start + fragment.length;\r\n      sentences.push({ sentence: fragment, start, end });\r\n      offset = end;\r\n    }\r\n    offset += 2;\r\n  }\r\n  if (!sentences.length && text.trim()) {\r\n    sentences.push({ sentence: text.trim(), start: 0, end: text.length });\r\n  }\r\n  return sentences;\r\n}\r\n\r\nexport function splitIntoParagraphs(text: string): SentenceBoundary[] {\r\n  const paragraphs: SentenceBoundary[] = [];\r\n  let cursor = 0;\r\n  for (const segment of text.split(PARAGRAPH_REGEX)) {\r\n    const trimmed = segment.trim();\r\n    if (!trimmed) {\r\n      cursor += segment.length + 2;\r\n      continue;\r\n    }\r\n    const start = text.indexOf(trimmed, cursor);\r\n    const end = start + trimmed.length;\r\n    paragraphs.push({ sentence: trimmed, start, end });\r\n    cursor = end + 2;\r\n  }\r\n  if (!paragraphs.length && text.trim()) {\r\n    paragraphs.push({ sentence: text.trim(), start: 0, end: text.length });\r\n  }\r\n  return paragraphs;\r\n}\r\n","import { v4 as uuid } from \"uuid\";\r\nimport { Chunk, Chunker, ChunkerOptions } from \"../types\";\r\nimport { splitIntoParagraphs, splitIntoSentences, tokenize } from \"../utils/tokenizer\";\r\n\r\nexport class RecursiveChunker implements Chunker {\r\n  private readonly chunkSize: number;\r\n\r\n  constructor(options: ChunkerOptions) {\r\n    this.chunkSize = options.chunkSize ?? 500;\r\n  }\r\n\r\n  async chunk(text: string): Promise<Chunk[]> {\r\n    const paragraphs = splitIntoParagraphs(text);\r\n    const chunks: Chunk[] = [];\r\n\r\n    for (const paragraph of paragraphs) {\r\n      if (this.tokenCount(paragraph.sentence) <= this.chunkSize) {\r\n        chunks.push({ id: uuid(), text: paragraph.sentence, start: paragraph.start, end: paragraph.end });\r\n      } else {\r\n        chunks.push(...this.splitParagraph(paragraph.sentence, paragraph.start));\r\n      }\r\n    }\r\n\r\n    return this.mergeSmallChunks(chunks);\r\n  }\r\n\r\n  private splitParagraph(paragraph: string, offset: number): Chunk[] {\r\n    const sentences = splitIntoSentences(paragraph);\r\n    const chunks: Chunk[] = [];\r\n    let current: string[] = [];\r\n    let start = offset;\r\n\r\n    for (const sentence of sentences) {\r\n      const tentative = [...current, sentence.sentence].join(\" \");\r\n      if (this.tokenCount(tentative) > this.chunkSize && current.length) {\r\n        const text = current.join(\" \");\r\n        const chunkStart = start;\r\n        const chunkEnd = chunkStart + text.length;\r\n        chunks.push({ id: uuid(), text, start: chunkStart, end: chunkEnd });\r\n        current = [sentence.sentence];\r\n        start = offset + sentence.start;\r\n      } else {\r\n        if (!current.length) {\r\n          start = offset + sentence.start;\r\n        }\r\n        current.push(sentence.sentence);\r\n      }\r\n    }\r\n\r\n    if (current.length) {\r\n      const text = current.join(\" \");\r\n      const chunkStart = start;\r\n      const chunkEnd = chunkStart + text.length;\r\n      chunks.push({ id: uuid(), text, start: chunkStart, end: chunkEnd });\r\n    }\r\n\r\n    return chunks.flatMap((chunk) =>\r\n      this.tokenCount(chunk.text) > this.chunkSize\r\n        ? this.splitTokens(chunk.text, chunk.start)\r\n        : chunk\r\n    );\r\n  }\r\n\r\n  private splitTokens(text: string, offset: number): Chunk[] {\r\n    const tokens = tokenize(text);\r\n    const chunks: Chunk[] = [];\r\n    for (let i = 0; i < tokens.length; i += this.chunkSize) {\r\n      const slice = tokens.slice(i, i + this.chunkSize);\r\n      const chunkText = slice.map((token) => token.value).join(\" \");\r\n      const start = offset + slice[0].start;\r\n      const end = offset + slice[slice.length - 1].end;\r\n      chunks.push({ id: uuid(), text: chunkText, start, end });\r\n    }\r\n    return chunks;\r\n  }\r\n\r\n  private mergeSmallChunks(chunks: Chunk[]): Chunk[] {\r\n    if (!chunks.length) return [];\r\n    const merged: Chunk[] = [];\r\n    let buffer = chunks[0];\r\n\r\n    for (let i = 1; i < chunks.length; i++) {\r\n      const candidate = chunks[i];\r\n      if (this.tokenCount(buffer.text + \" \" + candidate.text) <= this.chunkSize) {\r\n        buffer = {\r\n          id: buffer.id,\r\n          text: `${buffer.text} ${candidate.text}`.trim(),\r\n          start: buffer.start,\r\n          end: candidate.end\r\n        };\r\n      } else {\r\n        merged.push(buffer);\r\n        buffer = candidate;\r\n      }\r\n    }\r\n    merged.push(buffer);\r\n    return merged;\r\n  }\r\n\r\n  private tokenCount(text: string): number {\r\n    return tokenize(text).length;\r\n  }\r\n}\r\n","import { v4 as uuid } from \"uuid\";\r\nimport { Chunk, Chunker, ChunkerOptions, Embedder, ThresholdingStrategy } from \"../types\";\r\nimport { splitIntoSentences, tokenize } from \"../utils/tokenizer\";\r\nimport { pairwiseCosine } from \"../utils/cosine\";\r\nimport { percentile as percentileValue, zScoreThreshold } from \"../utils/stats\";\r\nimport { movingAverage } from \"../utils/smoothing\";\r\nimport { textTilingBoundaries } from \"../algorithms/TextTiling\";\r\nimport { c99Boundaries } from \"../algorithms/C99\";\r\nimport { bayesSegBoundaries } from \"../algorithms/BayesSeg\";\r\n\r\nclass LexicalEmbedder implements Embedder {\r\n  async embed(texts: string[]): Promise<number[][]> {\r\n    const dimension = 256;\r\n    return texts.map((text) => {\r\n      const vector = new Array(dimension).fill(0);\r\n      const tokens = tokenize(text.toLowerCase());\r\n      tokens.forEach((token) => {\r\n        const hash = this.hash(token.value) % dimension;\r\n        vector[hash] += 1;\r\n      });\r\n      return vector;\r\n    });\r\n  }\r\n\r\n  private hash(value: string): number {\r\n    let hash = 0;\r\n    for (let i = 0; i < value.length; i++) {\r\n      hash = (hash << 5) - hash + value.charCodeAt(i);\r\n      hash |= 0;\r\n    }\r\n    return Math.abs(hash);\r\n  }\r\n}\r\n\r\nexport class SemanticChunker implements Chunker {\r\n  private readonly chunkSize: number;\r\n  private readonly strategy: ThresholdingStrategy;\r\n  private readonly embedder: Embedder;\r\n  private readonly zScoreK: number;\r\n  private readonly smoothingWindow: number;\r\n  private readonly percentile: number;\r\n  private readonly gradientThreshold: number;\r\n\r\n  constructor(options: ChunkerOptions) {\r\n    this.chunkSize = options.chunkSize ?? 600;\r\n    this.strategy = options.thresholding ?? \"zscore\";\r\n    this.embedder = options.embedder ?? new LexicalEmbedder();\r\n    this.zScoreK = options.zScoreK ?? 1.0;\r\n    this.smoothingWindow = options.smoothingWindow ?? 3;\r\n    this.percentile = options.percentile ?? 10;\r\n    this.gradientThreshold = options.gradientThreshold ?? 0.15;\r\n  }\r\n\r\n  async chunk(text: string): Promise<Chunk[]> {\r\n    const sentences = splitIntoSentences(text);\r\n    if (sentences.length === 0) return [];\r\n    if (sentences.length === 1) {\r\n      return [\r\n        {\r\n          id: uuid(),\r\n          text: sentences[0].sentence,\r\n          start: sentences[0].start,\r\n          end: sentences[0].end\r\n        }\r\n      ];\r\n    }\r\n\r\n    const embeddings = await this.embedder.embed(sentences.map((sentence) => sentence.sentence));\r\n    const similarities = pairwiseCosine(embeddings);\r\n    const smoothed = movingAverage(similarities, this.smoothingWindow);\r\n    const boundaryIndexes = await this.computeBoundaries(text, sentences.map((s) => s.sentence), smoothed);\r\n\r\n    const chunks: Chunk[] = [];\r\n    let lastIndex = 0;\r\n    for (const index of boundaryIndexes) {\r\n      const startSentence = sentences[lastIndex];\r\n      const endSentence = sentences[index];\r\n      const chunkText = sentences.slice(lastIndex, index + 1).map((sentence) => sentence.sentence).join(\" \");\r\n      chunks.push({ id: uuid(), text: chunkText, start: startSentence.start, end: endSentence.end });\r\n      lastIndex = index + 1;\r\n    }\r\n\r\n    if (lastIndex < sentences.length) {\r\n      const startSentence = sentences[lastIndex];\r\n      const endSentence = sentences[sentences.length - 1];\r\n      const chunkText = sentences.slice(lastIndex).map((sentence) => sentence.sentence).join(\" \");\r\n      chunks.push({ id: uuid(), text: chunkText, start: startSentence.start, end: endSentence.end });\r\n    }\r\n\r\n    return this.enforceChunkSize(chunks);\r\n  }\r\n\r\n  private async computeBoundaries(text: string, sentences: string[], similarities: number[]): Promise<number[]> {\r\n    switch (this.strategy) {\r\n      case \"zscore\":\r\n        return this.zScoreBoundaries(similarities);\r\n      case \"percentile\":\r\n        return this.percentileBoundaries(similarities);\r\n      case \"localMinima\":\r\n        return this.localMinimaBoundaries(similarities);\r\n      case \"gradient\":\r\n        return this.gradientBoundaries(similarities);\r\n      case \"texttiling\":\r\n        return this.toSentenceIndexes(text, textTilingBoundaries(text));\r\n      case \"c99\":\r\n        return this.toSentenceIndexes(text, await c99Boundaries(text, this.embedder));\r\n      case \"bayesian\":\r\n        return this.toSentenceIndexes(text, bayesSegBoundaries(text));\r\n      default:\r\n        return this.zScoreBoundaries(similarities);\r\n    }\r\n  }\r\n\r\n  private zScoreBoundaries(similarities: number[]): number[] {\r\n    const threshold = zScoreThreshold(similarities, this.zScoreK);\r\n    return similarities\r\n      .map((value, index) => ({ value, index }))\r\n      .filter(({ value }) => value < threshold)\r\n      .map(({ index }) => index);\r\n  }\r\n\r\n  private percentileBoundaries(similarities: number[]): number[] {\r\n    const threshold = percentileValue(similarities, this.percentile);\r\n    return similarities\r\n      .map((value, index) => ({ value, index }))\r\n      .filter(({ value }) => value <= threshold)\r\n      .map(({ index }) => index);\r\n  }\r\n\r\n  private localMinimaBoundaries(similarities: number[]): number[] {\r\n    const boundaries: number[] = [];\r\n    for (let i = 1; i < similarities.length - 1; i++) {\r\n      if (similarities[i] < similarities[i - 1] && similarities[i] < similarities[i + 1]) {\r\n        boundaries.push(i);\r\n      }\r\n    }\r\n    return boundaries;\r\n  }\r\n\r\n  private gradientBoundaries(similarities: number[]): number[] {\r\n    const boundaries: number[] = [];\r\n    for (let i = 0; i < similarities.length - 1; i++) {\r\n      const gradient = similarities[i + 1] - similarities[i];\r\n      if (gradient < -this.gradientThreshold) {\r\n        boundaries.push(i);\r\n      }\r\n    }\r\n    return boundaries;\r\n  }\r\n\r\n  private toSentenceIndexes(text: string, boundaryPositions: number[]): number[] {\r\n    const sentences = splitIntoSentences(text);\r\n    const indexes: number[] = [];\r\n    boundaryPositions.forEach((position) => {\r\n      const index = sentences.findIndex((sentence) => sentence.end >= position);\r\n      if (index !== -1) {\r\n        indexes.push(index);\r\n      }\r\n    });\r\n    return indexes;\r\n  }\r\n\r\n  private enforceChunkSize(chunks: Chunk[]): Chunk[] {\r\n    const result: Chunk[] = [];\r\n    let buffer: Chunk | null = null;\r\n    for (const chunk of chunks) {\r\n      if (tokenize(chunk.text).length > this.chunkSize * 1.5) {\r\n        result.push(...this.splitLargeChunk(chunk));\r\n        continue;\r\n      }\r\n      if (!buffer) {\r\n        buffer = chunk;\r\n        continue;\r\n      }\r\n      const combined = `${buffer.text} ${chunk.text}`.trim();\r\n      if (tokenize(combined).length <= this.chunkSize) {\r\n        buffer = {\r\n          id: buffer.id,\r\n          text: combined,\r\n          start: buffer.start,\r\n          end: chunk.end\r\n        };\r\n      } else {\r\n        result.push(buffer);\r\n        buffer = chunk;\r\n      }\r\n    }\r\n    if (buffer) {\r\n      result.push(buffer);\r\n    }\r\n    return result;\r\n  }\r\n\r\n  private splitLargeChunk(chunk: Chunk): Chunk[] {\r\n    const tokens = tokenize(chunk.text);\r\n    const pieces: Chunk[] = [];\r\n    for (let i = 0; i < tokens.length; i += this.chunkSize) {\r\n      const slice = tokens.slice(i, i + this.chunkSize);\r\n      const text = slice.map((token) => token.value).join(\" \");\r\n      const startOffset = slice[0].start;\r\n      const endOffset = slice[slice.length - 1].end;\r\n      pieces.push({\r\n        id: uuid(),\r\n        text,\r\n        start: chunk.start + startOffset,\r\n        end: chunk.start + endOffset\r\n      });\r\n    }\r\n    return pieces;\r\n  }\r\n}\r\n","export function dot(a: number[], b: number[]): number {\r\n  return a.reduce((sum, value, index) => sum + value * (b[index] ?? 0), 0);\r\n}\r\n\r\nexport function magnitude(vec: number[]): number {\r\n  return Math.sqrt(vec.reduce((sum, value) => sum + value * value, 0));\r\n}\r\n\r\nexport function cosineSimilarity(a: number[], b: number[]): number {\r\n  const denom = magnitude(a) * magnitude(b);\r\n  if (denom === 0) return 0;\r\n  return dot(a, b) / denom;\r\n}\r\n\r\nexport function pairwiseCosine(vectors: number[][]): number[] {\r\n  const sims: number[] = [];\r\n  for (let i = 0; i < vectors.length - 1; i++) {\r\n    sims.push(cosineSimilarity(vectors[i], vectors[i + 1]));\r\n  }\r\n  return sims;\r\n}\r\n","export function mean(values: number[]): number {\r\n  if (!values.length) return 0;\r\n  return values.reduce((sum, value) => sum + value, 0) / values.length;\r\n}\r\n\r\nexport function stdev(values: number[]): number {\r\n  if (values.length < 2) return 0;\r\n  const avg = mean(values);\r\n  const variance = values.reduce((sum, value) => sum + Math.pow(value - avg, 2), 0) / (values.length - 1);\r\n  return Math.sqrt(variance);\r\n}\r\n\r\nexport function percentile(values: number[], percentileValue: number): number {\r\n  if (!values.length) return 0;\r\n  const sorted = [...values].sort((a, b) => a - b);\r\n  const index = Math.min(sorted.length - 1, Math.floor((percentileValue / 100) * sorted.length));\r\n  return sorted[index];\r\n}\r\n\r\nexport function zScoreThreshold(values: number[], k: number): number {\r\n  const avg = mean(values);\r\n  const deviation = stdev(values);\r\n  return avg - k * deviation;\r\n}\r\n","export function movingAverage(values: number[], window: number): number[] {\r\n  if (window <= 1) return [...values];\r\n  const result: number[] = [];\r\n  for (let i = 0; i < values.length; i++) {\r\n    const start = Math.max(0, i - Math.floor(window / 2));\r\n    const end = Math.min(values.length, i + Math.ceil(window / 2));\r\n    const slice = values.slice(start, end);\r\n    const average = slice.reduce((sum, value) => sum + value, 0) / slice.length;\r\n    result.push(average);\r\n  }\r\n  return result;\r\n}\r\n\r\nexport function normalize(values: number[]): number[] {\r\n  if (!values.length) return [];\r\n  const min = Math.min(...values);\r\n  const max = Math.max(...values);\r\n  if (max === min) return values.map(() => 0);\r\n  return values.map((value) => (value - min) / (max - min));\r\n}\r\n","import { movingAverage, normalize } from \"../utils/smoothing\";\r\nimport { splitIntoSentences, tokenize } from \"../utils/tokenizer\";\r\n\r\nexport interface TextTilingOptions {\r\n  blockSize?: number;\r\n  smoothingWindow?: number;\r\n  depthThreshold?: number;\r\n}\r\n\r\nexport function textTilingBoundaries(text: string, options: TextTilingOptions = {}): number[] {\r\n  const { blockSize = 6, smoothingWindow = 4, depthThreshold = 0.1 } = options;\r\n  const sentences = splitIntoSentences(text);\r\n  if (sentences.length <= 1) return [];\r\n\r\n  const lexicalScores: number[] = [];\r\n  for (let i = 0; i < sentences.length - 1; i++) {\r\n    const leftTokens = tokenize(sentences[Math.max(0, i - blockSize + 1)].sentence).map((token) => token.value.toLowerCase());\r\n    const rightTokens = tokenize(sentences[Math.min(sentences.length - 1, i + blockSize)].sentence).map((token) => token.value.toLowerCase());\r\n    const overlap = leftTokens.filter((token) => rightTokens.includes(token)).length;\r\n    const score = overlap / Math.max(1, Math.sqrt(leftTokens.length * rightTokens.length));\r\n    lexicalScores.push(score);\r\n  }\r\n\r\n  const smoothed = movingAverage(lexicalScores, smoothingWindow);\r\n  const normalized = normalize(smoothed);\r\n\r\n  const boundaries: number[] = [];\r\n  for (let i = 1; i < normalized.length - 1; i++) {\r\n    const depth = normalized[i - 1] - normalized[i] + (normalized[i + 1] - normalized[i]);\r\n    if (depth > depthThreshold && normalized[i] < normalized[i - 1] && normalized[i] < normalized[i + 1]) {\r\n      boundaries.push(sentences[i].end);\r\n    }\r\n  }\r\n  return boundaries;\r\n}\r\n","import { cosineSimilarity } from \"../utils/cosine\";\r\nimport { splitIntoSentences } from \"../utils/tokenizer\";\r\nimport type { Embedder } from \"../types\";\r\n\r\nexport interface C99Options {\r\n  window?: number;\r\n  smoothingWindow?: number;\r\n}\r\n\r\nexport async function c99Boundaries(text: string, embedder: Embedder, options: C99Options = {}): Promise<number[]> {\r\n  const { window = 5 } = options;\r\n  const sentences = splitIntoSentences(text);\r\n  if (sentences.length <= 1) return [];\r\n  const vectors = await embedder.embed(sentences.map((s) => s.sentence));\r\n  const matrix: number[][] = Array.from({ length: vectors.length }, (_, i) =>\r\n    vectors.map((vec) => cosineSimilarity(vec, vectors[i]))\r\n  );\r\n\r\n  const boundaries: number[] = [];\r\n  for (let i = window; i < matrix.length - window; i++) {\r\n    let leftVariance = 0;\r\n    let rightVariance = 0;\r\n\r\n    for (let j = i - window; j < i; j++) {\r\n      leftVariance += 1 - matrix[i][j];\r\n    }\r\n    for (let j = i; j < i + window; j++) {\r\n      rightVariance += 1 - matrix[i][j];\r\n    }\r\n\r\n    const contrast = Math.abs(leftVariance - rightVariance) / window;\r\n    if (contrast > 0.2) {\r\n      boundaries.push(sentences[i].end);\r\n    }\r\n  }\r\n\r\n  return boundaries;\r\n}\r\n","import { splitIntoSentences, tokenize } from \"../utils/tokenizer\";\r\n\r\nexport interface BayesSegOptions {\r\n  maxSegments?: number;\r\n  minSegmentLength?: number;\r\n}\r\n\r\nfunction sentenceVector(sentence: string): Map<string, number> {\r\n  const tokens = tokenize(sentence.toLowerCase());\r\n  const freq = new Map<string, number>();\r\n  for (const token of tokens) {\r\n    freq.set(token.value, (freq.get(token.value) ?? 0) + 1);\r\n  }\r\n  return freq;\r\n}\r\n\r\nfunction logLikelihood(sentences: string[], start: number, end: number): number {\r\n  const counts = new Map<string, number>();\r\n  let total = 0;\r\n  for (let i = start; i < end; i++) {\r\n    const vec = sentenceVector(sentences[i]);\r\n    for (const [token, count] of vec.entries()) {\r\n      counts.set(token, (counts.get(token) ?? 0) + count);\r\n      total += count;\r\n    }\r\n  }\r\n  let logProb = 0;\r\n  for (const [, count] of counts.entries()) {\r\n    const p = count / total;\r\n    logProb += count * Math.log(p);\r\n  }\r\n  return logProb;\r\n}\r\n\r\nexport function bayesSegBoundaries(text: string, options: BayesSegOptions = {}): number[] {\r\n  const { maxSegments = 5, minSegmentLength = 2 } = options;\r\n  const sentences = splitIntoSentences(text);\r\n  if (sentences.length <= 1) return [];\r\n\r\n  const dp: number[] = Array(sentences.length + 1).fill(-Infinity);\r\n  const backtrack: number[] = Array(sentences.length + 1).fill(-1);\r\n  dp[0] = 0;\r\n\r\n  for (let i = minSegmentLength; i <= sentences.length; i++) {\r\n    for (let j = Math.max(0, i - 20); j <= i - minSegmentLength; j++) {\r\n      const segmentCount = (backtrack[j] === -1 ? 0 : 1) + 1;\r\n      if (segmentCount > maxSegments) continue;\r\n      const score = dp[j] + logLikelihood(sentences.map((s) => s.sentence), j, i);\r\n      if (score > dp[i]) {\r\n        dp[i] = score;\r\n        backtrack[i] = j;\r\n      }\r\n    }\r\n  }\r\n\r\n  const boundaries: number[] = [];\r\n  let idx = sentences.length;\r\n  while (idx > 0 && backtrack[idx] !== -1) {\r\n    boundaries.push(sentences[idx - 1].end);\r\n    idx = backtrack[idx];\r\n  }\r\n\r\n  return boundaries.reverse();\r\n}\r\n","import { v4 as uuid } from \"uuid\";\r\nimport { Chunk, Chunker, ChunkerOptions } from \"../types\";\r\nimport { splitIntoParagraphs, tokenize } from \"../utils/tokenizer\";\r\nimport { SemanticChunker } from \"./SemanticChunker\";\r\n\r\nexport class HybridChunker implements Chunker {\r\n  private readonly semantic: SemanticChunker;\r\n  private readonly minChunkSize: number;\r\n  private readonly maxChunkSize: number;\r\n\r\n  constructor(options: ChunkerOptions) {\r\n    this.semantic = new SemanticChunker({ ...options, type: \"semantic\" });\r\n    this.minChunkSize = options.minChunkSize ?? Math.floor((options.chunkSize ?? 600) / 2);\r\n    this.maxChunkSize = options.maxChunkSize ?? (options.chunkSize ?? 600) * 1.5;\r\n  }\r\n\r\n  async chunk(text: string): Promise<Chunk[]> {\r\n    const paragraphs = splitIntoParagraphs(text);\r\n    const chunks: Chunk[] = [];\r\n\r\n    for (const paragraph of paragraphs) {\r\n      const tokens = tokenize(paragraph.sentence);\r\n      if (tokens.length <= this.minChunkSize) {\r\n        chunks.push({ id: uuid(), text: paragraph.sentence, start: paragraph.start, end: paragraph.end });\r\n      } else {\r\n        const semanticChunks = await this.semantic.chunk(paragraph.sentence);\r\n        semanticChunks.forEach((chunk) => {\r\n          chunks.push({\r\n            ...chunk,\r\n            start: paragraph.start + chunk.start,\r\n            end: paragraph.start + chunk.end\r\n          });\r\n        });\r\n      }\r\n    }\r\n\r\n    return this.balanceChunks(chunks);\r\n  }\r\n\r\n  private balanceChunks(chunks: Chunk[]): Chunk[] {\r\n    const balanced: Chunk[] = [];\r\n    let buffer: Chunk | null = null;\r\n\r\n    for (const chunk of chunks) {\r\n      if (!buffer) {\r\n        buffer = chunk;\r\n        continue;\r\n      }\r\n      const combined = `${buffer.text}\\n${chunk.text}`.trim();\r\n      const combinedTokens = tokenize(combined).length;\r\n      if (combinedTokens < this.minChunkSize) {\r\n        buffer = {\r\n          id: buffer.id,\r\n          text: combined,\r\n          start: buffer.start,\r\n          end: chunk.end\r\n        };\r\n      } else if (combinedTokens > this.maxChunkSize) {\r\n        balanced.push(buffer);\r\n        buffer = chunk;\r\n      } else {\r\n        buffer = {\r\n          id: buffer.id,\r\n          text: combined,\r\n          start: buffer.start,\r\n          end: chunk.end\r\n        };\r\n      }\r\n    }\r\n\r\n    if (buffer) {\r\n      balanced.push(buffer);\r\n    }\r\n\r\n    return balanced;\r\n  }\r\n}\r\n","import { v4 as uuid } from \"uuid\";\r\nimport { Chunk, Chunker, ChunkerOptions, Embedder } from \"../types\";\r\nimport { splitIntoSentences } from \"../utils/tokenizer\";\r\nimport { tokenize } from \"../utils/tokenizer\";\r\n\r\nclass LexicalTopicEmbedder implements Embedder {\r\n  async embed(texts: string[]): Promise<number[][]> {\r\n    const dimension = 256;\r\n    return texts.map((text) => {\r\n      const vector = new Array(dimension).fill(0);\r\n      const tokens = tokenize(text.toLowerCase());\r\n      tokens.forEach((token) => {\r\n        const hash = this.hash(token.value) % dimension;\r\n        vector[hash] += 1;\r\n      });\r\n      return vector;\r\n    });\r\n  }\r\n\r\n  private hash(value: string): number {\r\n    let hash = 0;\r\n    for (let i = 0; i < value.length; i++) {\r\n      hash = (hash << 5) - hash + value.charCodeAt(i);\r\n      hash |= 0;\r\n    }\r\n    return Math.abs(hash);\r\n  }\r\n}\r\n\r\nexport class TopicChunker implements Chunker {\r\n  private readonly embedder: Embedder;\r\n  private readonly topicCount?: number;\r\n\r\n  constructor(options: ChunkerOptions) {\r\n    this.embedder = options.embedder ?? new LexicalTopicEmbedder();\r\n    this.topicCount = options.topicCount;\r\n  }\r\n\r\n  async chunk(text: string): Promise<Chunk[]> {\r\n    const sentences = splitIntoSentences(text);\r\n    if (!sentences.length) return [];\r\n    const vectors = await this.embedder.embed(sentences.map((s) => s.sentence));\r\n    const k = Math.min(\r\n      sentences.length,\r\n      this.topicCount ?? Math.max(2, Math.round(Math.sqrt(sentences.length)))\r\n    );\r\n    const assignments = this.kMeans(vectors, k);\r\n\r\n    const chunks: Chunk[] = [];\r\n    let currentCluster = assignments[0];\r\n    let startIndex = 0;\r\n    for (let i = 1; i < assignments.length; i++) {\r\n      if (assignments[i] !== currentCluster) {\r\n        const startSentence = sentences[startIndex];\r\n        const endSentence = sentences[i - 1];\r\n        const chunkText = sentences.slice(startIndex, i).map((s) => s.sentence).join(\" \");\r\n        chunks.push({ id: uuid(), text: chunkText, start: startSentence.start, end: endSentence.end, metadata: { cluster: currentCluster } });\r\n        startIndex = i;\r\n        currentCluster = assignments[i];\r\n      }\r\n    }\r\n\r\n    const startSentence = sentences[startIndex];\r\n    const endSentence = sentences[sentences.length - 1];\r\n    const chunkText = sentences.slice(startIndex).map((s) => s.sentence).join(\" \");\r\n    chunks.push({ id: uuid(), text: chunkText, start: startSentence.start, end: endSentence.end, metadata: { cluster: currentCluster } });\r\n\r\n    return chunks;\r\n  }\r\n\r\n  private kMeans(vectors: number[][], k: number, iterations = 20): number[] {\r\n    const centroids = vectors.slice(0, k).map((vec) => [...vec]);\r\n    const assignments = new Array(vectors.length).fill(0);\r\n\r\n    for (let iteration = 0; iteration < iterations; iteration++) {\r\n      let changed = false;\r\n      for (let i = 0; i < vectors.length; i++) {\r\n        let bestIndex = 0;\r\n        let bestDistance = Infinity;\r\n        for (let c = 0; c < k; c++) {\r\n          const distance = this.euclideanDistance(vectors[i], centroids[c]);\r\n          if (distance < bestDistance) {\r\n            bestDistance = distance;\r\n            bestIndex = c;\r\n          }\r\n        }\r\n        if (assignments[i] !== bestIndex) {\r\n          assignments[i] = bestIndex;\r\n          changed = true;\r\n        }\r\n      }\r\n\r\n      if (!changed) break;\r\n\r\n      const sums = Array.from({ length: k }, () => new Array(vectors[0].length).fill(0));\r\n      const counts = new Array(k).fill(0);\r\n      vectors.forEach((vector, index) => {\r\n        const cluster = assignments[index];\r\n        counts[cluster]++;\r\n        for (let d = 0; d < vector.length; d++) {\r\n          sums[cluster][d] += vector[d];\r\n        }\r\n      });\r\n      for (let c = 0; c < k; c++) {\r\n        if (counts[c] === 0) continue;\r\n        centroids[c] = sums[c].map((value) => value / counts[c]);\r\n      }\r\n    }\r\n\r\n    return assignments;\r\n  }\r\n\r\n  private euclideanDistance(a: number[], b: number[]): number {\r\n    const length = Math.max(a.length, b.length);\r\n    let sum = 0;\r\n    for (let i = 0; i < length; i++) {\r\n      const diff = (a[i] ?? 0) - (b[i] ?? 0);\r\n      sum += diff * diff;\r\n    }\r\n    return Math.sqrt(sum);\r\n  }\r\n}\r\n","import { v4 as uuid } from \"uuid\";\r\nimport { Chunk, Chunker, ChunkerOptions } from \"../types\";\r\nimport { splitIntoSentences, tokenize } from \"../utils/tokenizer\";\r\n\r\nexport class SlidingWindowChunker implements Chunker {\r\n  private readonly chunkSize: number;\r\n  private readonly overlap: number;\r\n\r\n  constructor(options: ChunkerOptions) {\r\n    this.chunkSize = options.chunkSize ?? 400;\r\n    this.overlap = options.chunkOverlap ?? Math.floor(this.chunkSize / 4);\r\n  }\r\n\r\n  async chunk(text: string): Promise<Chunk[]> {\r\n    const sentences = splitIntoSentences(text);\r\n    if (!sentences.length) return [];\r\n\r\n    const tokens = tokenize(text);\r\n    if (!tokens.length) return [];\r\n\r\n    const stride = Math.max(1, this.chunkSize - this.overlap);\r\n    const chunks: Chunk[] = [];\r\n\r\n    for (let start = 0; start < tokens.length; start += stride) {\r\n      const end = Math.min(tokens.length, start + this.chunkSize);\r\n      const chunkTokens = tokens.slice(start, end);\r\n      const chunkText = chunkTokens.map((token) => token.value).join(\" \").trim();\r\n      const chunkStart = chunkTokens[0].start;\r\n      const chunkEnd = chunkTokens[chunkTokens.length - 1].end;\r\n\r\n      const boundaryAdjusted = this.expandToSentenceBoundaries(chunkStart, chunkEnd, sentences);\r\n      chunks.push({ id: uuid(), text: chunkText, start: boundaryAdjusted.start, end: boundaryAdjusted.end });\r\n      if (end === tokens.length) break;\r\n    }\r\n\r\n    return chunks;\r\n  }\r\n\r\n  private expandToSentenceBoundaries(start: number, end: number, sentences: ReturnType<typeof splitIntoSentences>): { start: number; end: number } {\r\n    let expandedStart = start;\r\n    let expandedEnd = end;\r\n    for (const sentence of sentences) {\r\n      if (sentence.start <= start && sentence.end >= start) {\r\n        expandedStart = sentence.start;\r\n      }\r\n      if (sentence.start <= end && sentence.end >= end) {\r\n        expandedEnd = sentence.end;\r\n      }\r\n    }\r\n    return { start: expandedStart, end: expandedEnd };\r\n  }\r\n}\r\n","import { ChunkerOptions, ChunkerType } from \"../types\";\r\nimport { FixedChunker } from \"../chunkers/FixedChunker\";\r\nimport { RecursiveChunker } from \"../chunkers/RecursiveChunker\";\r\nimport { SemanticChunker } from \"../chunkers/SemanticChunker\";\r\nimport { HybridChunker } from \"../chunkers/HybridChunker\";\r\nimport { TopicChunker } from \"../chunkers/TopicChunker\";\r\nimport { SlidingWindowChunker } from \"../chunkers/SlidingWindowChunker\";\r\nimport type { Chunker } from \"../types\";\r\n\r\nconst registry = new Map<ChunkerType | string, (options: ChunkerOptions) => Chunker>();\r\n\r\nregistry.set(\"fixed\", (options) => new FixedChunker(options));\r\nregistry.set(\"recursive\", (options) => new RecursiveChunker(options));\r\nregistry.set(\"semantic\", (options) => new SemanticChunker(options));\r\nregistry.set(\"hybrid\", (options) => new HybridChunker(options));\r\nregistry.set(\"topic\", (options) => new TopicChunker(options));\r\nregistry.set(\"sliding\", (options) => new SlidingWindowChunker(options));\r\n\r\nexport function registerChunker(type: string, factory: (options: ChunkerOptions) => Chunker) {\r\n  registry.set(type, factory);\r\n}\r\n\r\nexport function createChunker(options: ChunkerOptions): Chunker {\r\n  const factory = registry.get(options.type);\r\n  if (!factory) {\r\n    throw new Error(`Unknown chunker type: ${options.type}`);\r\n  }\r\n  return factory(options);\r\n}\r\n","import type { Embedder } from \"../types\";\r\n\r\nexport interface OpenAIEmbedderOptions {\r\n  apiKey?: string;\r\n  model?: string;\r\n  endpoint?: string;\r\n}\r\n\r\nexport class OpenAIEmbedder implements Embedder {\r\n  private readonly apiKey: string;\r\n  private readonly model: string;\r\n  private readonly endpoint: string;\r\n\r\n  constructor(options: OpenAIEmbedderOptions = {}) {\r\n    this.apiKey = options.apiKey ?? process.env.OPENAI_API_KEY ?? \"\";\r\n    this.model = options.model ?? \"text-embedding-3-small\";\r\n    this.endpoint = options.endpoint ?? \"https://api.openai.com/v1/embeddings\";\r\n    if (!this.apiKey) {\r\n      throw new Error(\"OpenAI API key is required for OpenAIEmbedder\");\r\n    }\r\n  }\r\n\r\n  async embed(texts: string[]): Promise<number[][]> {\r\n    const response = await fetch(this.endpoint, {\r\n      method: \"POST\",\r\n      headers: {\r\n        \"Content-Type\": \"application/json\",\r\n        Authorization: `Bearer ${this.apiKey}`\r\n      },\r\n      body: JSON.stringify({ input: texts, model: this.model })\r\n    });\r\n\r\n    if (!response.ok) {\r\n      const message = await response.text();\r\n      throw new Error(`OpenAI embedding request failed: ${message}`);\r\n    }\r\n\r\n    const data = (await response.json()) as { data: { embedding: number[] }[] };\r\n    return data.data.map((entry) => entry.embedding);\r\n  }\r\n}\r\n","import type { Embedder } from \"../types\";\r\n\r\nexport interface HuggingFaceEmbedderOptions {\r\n  apiKey?: string;\r\n  model?: string;\r\n  endpoint?: string;\r\n}\r\n\r\nexport class HuggingFaceEmbedder implements Embedder {\r\n  private readonly apiKey?: string;\r\n  private readonly model: string;\r\n  private readonly endpoint: string;\r\n\r\n  constructor(options: HuggingFaceEmbedderOptions = {}) {\r\n    this.apiKey = options.apiKey ?? process.env.HUGGINGFACE_API_KEY;\r\n    this.model = options.model ?? \"sentence-transformers/all-MiniLM-L6-v2\";\r\n    this.endpoint = options.endpoint ?? `https://api-inference.huggingface.co/pipeline/feature-extraction/${this.model}`;\r\n  }\r\n\r\n  async embed(texts: string[]): Promise<number[][]> {\r\n    const responses: number[][] = [];\r\n    for (const text of texts) {\r\n      const response = await fetch(this.endpoint, {\r\n        method: \"POST\",\r\n        headers: {\r\n          \"Content-Type\": \"application/json\",\r\n          ...(this.apiKey ? { Authorization: `Bearer ${this.apiKey}` } : {})\r\n        },\r\n        body: JSON.stringify({ inputs: text })\r\n      });\r\n      if (!response.ok) {\r\n        const message = await response.text();\r\n        throw new Error(`Hugging Face embedding request failed: ${message}`);\r\n      }\r\n      const data = (await response.json()) as number[][] | number[];\r\n      responses.push(Array.isArray(data[0]) ? (data as number[][])[0] : (data as number[]));\r\n    }\r\n    return responses;\r\n  }\r\n}\r\n","import type { Embedder } from \"../types\";\r\n\r\nexport type LocalEmbedFunction = (texts: string[]) => Promise<number[][]> | number[][];\r\n\r\nexport class LocalModelEmbedder implements Embedder {\r\n  constructor(private readonly embedFn: LocalEmbedFunction) {}\r\n\r\n  async embed(texts: string[]): Promise<number[][]> {\r\n    const result = await this.embedFn(texts);\r\n    return result;\r\n  }\r\n}\r\n"],"mappings":";AAAA,SAAS,MAAM,YAAY;;;ACE3B,IAAM,iBAAiB;AACvB,IAAM,kBAAkB;AAQjB,SAAS,SAAS,MAAuB;AAC9C,QAAM,SAAkB,CAAC;AACzB,MAAI,QAAQ;AACZ,aAAW,QAAQ,KAAK,MAAM,OAAO,GAAG;AACtC,QAAI,CAAC;AAAM;AACX,UAAM,QAAQ;AACd,UAAM,MAAM,QAAQ,KAAK;AACzB,WAAO,KAAK,EAAE,OAAO,MAAM,OAAO,IAAI,CAAC;AACvC,YAAQ;AAAA,EACV;AACA,SAAO,OAAO,OAAO,CAAC,UAAU,MAAM,MAAM,KAAK,EAAE,SAAS,CAAC;AAC/D;AAMO,SAAS,mBAAmB,MAAkC;AACnE,QAAM,YAAgC,CAAC;AACvC,MAAI,SAAS;AACb,aAAW,aAAa,KAAK,MAAM,eAAe,GAAG;AACnD,UAAM,UAAU,UAAU,KAAK;AAC/B,QAAI,CAAC,SAAS;AACZ,gBAAU,UAAU,SAAS;AAC7B;AAAA,IACF;AACA,UAAM,YAAY,QAAQ,MAAM,cAAc;AAC9C,eAAW,YAAY,WAAW;AAChC,YAAM,QAAQ,KAAK,QAAQ,UAAU,MAAM;AAC3C,YAAM,MAAM,QAAQ,SAAS;AAC7B,gBAAU,KAAK,EAAE,UAAU,UAAU,OAAO,IAAI,CAAC;AACjD,eAAS;AAAA,IACX;AACA,cAAU;AAAA,EACZ;AACA,MAAI,CAAC,UAAU,UAAU,KAAK,KAAK,GAAG;AACpC,cAAU,KAAK,EAAE,UAAU,KAAK,KAAK,GAAG,OAAO,GAAG,KAAK,KAAK,OAAO,CAAC;AAAA,EACtE;AACA,SAAO;AACT;AAEO,SAAS,oBAAoB,MAAkC;AACpE,QAAM,aAAiC,CAAC;AACxC,MAAI,SAAS;AACb,aAAW,WAAW,KAAK,MAAM,eAAe,GAAG;AACjD,UAAM,UAAU,QAAQ,KAAK;AAC7B,QAAI,CAAC,SAAS;AACZ,gBAAU,QAAQ,SAAS;AAC3B;AAAA,IACF;AACA,UAAM,QAAQ,KAAK,QAAQ,SAAS,MAAM;AAC1C,UAAM,MAAM,QAAQ,QAAQ;AAC5B,eAAW,KAAK,EAAE,UAAU,SAAS,OAAO,IAAI,CAAC;AACjD,aAAS,MAAM;AAAA,EACjB;AACA,MAAI,CAAC,WAAW,UAAU,KAAK,KAAK,GAAG;AACrC,eAAW,KAAK,EAAE,UAAU,KAAK,KAAK,GAAG,OAAO,GAAG,KAAK,KAAK,OAAO,CAAC;AAAA,EACvE;AACA,SAAO;AACT;;;ADlEO,IAAM,eAAN,MAAsC;AAAA,EAI3C,YAAY,SAAyB;AACnC,SAAK,YAAY,QAAQ,aAAa;AACtC,SAAK,eAAe,QAAQ,gBAAgB,KAAK,MAAM,KAAK,YAAY,EAAE;AAAA,EAC5E;AAAA,EAEA,MAAM,MAAM,MAAgC;AAC1C,UAAM,SAAS,SAAS,IAAI;AAC5B,QAAI,CAAC,OAAO;AAAQ,aAAO,CAAC;AAC5B,UAAM,SAAkB,CAAC;AACzB,QAAI,aAAa;AAEjB,WAAO,aAAa,OAAO,QAAQ;AACjC,YAAM,WAAW,KAAK,IAAI,OAAO,QAAQ,aAAa,KAAK,SAAS;AACpE,YAAM,cAAc,OAAO,MAAM,YAAY,QAAQ;AACrD,YAAM,YAAY,YAAY,IAAI,CAAC,UAAU,MAAM,KAAK,EAAE,KAAK,GAAG,EAAE,KAAK;AACzE,YAAM,QAAQ,YAAY,CAAC,EAAE;AAC7B,YAAM,MAAM,YAAY,YAAY,SAAS,CAAC,EAAE;AAChD,aAAO,KAAK,EAAE,IAAI,KAAK,GAAG,MAAM,WAAW,OAAO,IAAI,CAAC;AACvD,UAAI,aAAa,OAAO;AAAQ;AAChC,mBAAa,KAAK,IAAI,GAAG,WAAW,KAAK,YAAY;AAAA,IACvD;AAEA,WAAO;AAAA,EACT;AACF;;;AEhCA,SAAS,MAAMA,aAAY;AAIpB,IAAM,mBAAN,MAA0C;AAAA,EAG/C,YAAY,SAAyB;AACnC,SAAK,YAAY,QAAQ,aAAa;AAAA,EACxC;AAAA,EAEA,MAAM,MAAM,MAAgC;AAC1C,UAAM,aAAa,oBAAoB,IAAI;AAC3C,UAAM,SAAkB,CAAC;AAEzB,eAAW,aAAa,YAAY;AAClC,UAAI,KAAK,WAAW,UAAU,QAAQ,KAAK,KAAK,WAAW;AACzD,eAAO,KAAK,EAAE,IAAIC,MAAK,GAAG,MAAM,UAAU,UAAU,OAAO,UAAU,OAAO,KAAK,UAAU,IAAI,CAAC;AAAA,MAClG,OAAO;AACL,eAAO,KAAK,GAAG,KAAK,eAAe,UAAU,UAAU,UAAU,KAAK,CAAC;AAAA,MACzE;AAAA,IACF;AAEA,WAAO,KAAK,iBAAiB,MAAM;AAAA,EACrC;AAAA,EAEQ,eAAe,WAAmB,QAAyB;AACjE,UAAM,YAAY,mBAAmB,SAAS;AAC9C,UAAM,SAAkB,CAAC;AACzB,QAAI,UAAoB,CAAC;AACzB,QAAI,QAAQ;AAEZ,eAAW,YAAY,WAAW;AAChC,YAAM,YAAY,CAAC,GAAG,SAAS,SAAS,QAAQ,EAAE,KAAK,GAAG;AAC1D,UAAI,KAAK,WAAW,SAAS,IAAI,KAAK,aAAa,QAAQ,QAAQ;AACjE,cAAM,OAAO,QAAQ,KAAK,GAAG;AAC7B,cAAM,aAAa;AACnB,cAAM,WAAW,aAAa,KAAK;AACnC,eAAO,KAAK,EAAE,IAAIA,MAAK,GAAG,MAAM,OAAO,YAAY,KAAK,SAAS,CAAC;AAClE,kBAAU,CAAC,SAAS,QAAQ;AAC5B,gBAAQ,SAAS,SAAS;AAAA,MAC5B,OAAO;AACL,YAAI,CAAC,QAAQ,QAAQ;AACnB,kBAAQ,SAAS,SAAS;AAAA,QAC5B;AACA,gBAAQ,KAAK,SAAS,QAAQ;AAAA,MAChC;AAAA,IACF;AAEA,QAAI,QAAQ,QAAQ;AAClB,YAAM,OAAO,QAAQ,KAAK,GAAG;AAC7B,YAAM,aAAa;AACnB,YAAM,WAAW,aAAa,KAAK;AACnC,aAAO,KAAK,EAAE,IAAIA,MAAK,GAAG,MAAM,OAAO,YAAY,KAAK,SAAS,CAAC;AAAA,IACpE;AAEA,WAAO,OAAO;AAAA,MAAQ,CAAC,UACrB,KAAK,WAAW,MAAM,IAAI,IAAI,KAAK,YAC/B,KAAK,YAAY,MAAM,MAAM,MAAM,KAAK,IACxC;AAAA,IACN;AAAA,EACF;AAAA,EAEQ,YAAY,MAAc,QAAyB;AACzD,UAAM,SAAS,SAAS,IAAI;AAC5B,UAAM,SAAkB,CAAC;AACzB,aAAS,IAAI,GAAG,IAAI,OAAO,QAAQ,KAAK,KAAK,WAAW;AACtD,YAAM,QAAQ,OAAO,MAAM,GAAG,IAAI,KAAK,SAAS;AAChD,YAAM,YAAY,MAAM,IAAI,CAAC,UAAU,MAAM,KAAK,EAAE,KAAK,GAAG;AAC5D,YAAM,QAAQ,SAAS,MAAM,CAAC,EAAE;AAChC,YAAM,MAAM,SAAS,MAAM,MAAM,SAAS,CAAC,EAAE;AAC7C,aAAO,KAAK,EAAE,IAAIA,MAAK,GAAG,MAAM,WAAW,OAAO,IAAI,CAAC;AAAA,IACzD;AACA,WAAO;AAAA,EACT;AAAA,EAEQ,iBAAiB,QAA0B;AACjD,QAAI,CAAC,OAAO;AAAQ,aAAO,CAAC;AAC5B,UAAM,SAAkB,CAAC;AACzB,QAAI,SAAS,OAAO,CAAC;AAErB,aAAS,IAAI,GAAG,IAAI,OAAO,QAAQ,KAAK;AACtC,YAAM,YAAY,OAAO,CAAC;AAC1B,UAAI,KAAK,WAAW,OAAO,OAAO,MAAM,UAAU,IAAI,KAAK,KAAK,WAAW;AACzE,iBAAS;AAAA,UACP,IAAI,OAAO;AAAA,UACX,MAAM,GAAG,OAAO,IAAI,IAAI,UAAU,IAAI,GAAG,KAAK;AAAA,UAC9C,OAAO,OAAO;AAAA,UACd,KAAK,UAAU;AAAA,QACjB;AAAA,MACF,OAAO;AACL,eAAO,KAAK,MAAM;AAClB,iBAAS;AAAA,MACX;AAAA,IACF;AACA,WAAO,KAAK,MAAM;AAClB,WAAO;AAAA,EACT;AAAA,EAEQ,WAAW,MAAsB;AACvC,WAAO,SAAS,IAAI,EAAE;AAAA,EACxB;AACF;;;ACtGA,SAAS,MAAMC,aAAY;;;ACApB,SAAS,IAAI,GAAa,GAAqB;AACpD,SAAO,EAAE,OAAO,CAAC,KAAK,OAAO,UAAU,MAAM,SAAS,EAAE,KAAK,KAAK,IAAI,CAAC;AACzE;AAEO,SAAS,UAAU,KAAuB;AAC/C,SAAO,KAAK,KAAK,IAAI,OAAO,CAAC,KAAK,UAAU,MAAM,QAAQ,OAAO,CAAC,CAAC;AACrE;AAEO,SAAS,iBAAiB,GAAa,GAAqB;AACjE,QAAM,QAAQ,UAAU,CAAC,IAAI,UAAU,CAAC;AACxC,MAAI,UAAU;AAAG,WAAO;AACxB,SAAO,IAAI,GAAG,CAAC,IAAI;AACrB;AAEO,SAAS,eAAe,SAA+B;AAC5D,QAAM,OAAiB,CAAC;AACxB,WAAS,IAAI,GAAG,IAAI,QAAQ,SAAS,GAAG,KAAK;AAC3C,SAAK,KAAK,iBAAiB,QAAQ,CAAC,GAAG,QAAQ,IAAI,CAAC,CAAC,CAAC;AAAA,EACxD;AACA,SAAO;AACT;;;ACpBO,SAAS,KAAK,QAA0B;AAC7C,MAAI,CAAC,OAAO;AAAQ,WAAO;AAC3B,SAAO,OAAO,OAAO,CAAC,KAAK,UAAU,MAAM,OAAO,CAAC,IAAI,OAAO;AAChE;AAEO,SAAS,MAAM,QAA0B;AAC9C,MAAI,OAAO,SAAS;AAAG,WAAO;AAC9B,QAAM,MAAM,KAAK,MAAM;AACvB,QAAM,WAAW,OAAO,OAAO,CAAC,KAAK,UAAU,MAAM,KAAK,IAAI,QAAQ,KAAK,CAAC,GAAG,CAAC,KAAK,OAAO,SAAS;AACrG,SAAO,KAAK,KAAK,QAAQ;AAC3B;AAEO,SAAS,WAAW,QAAkB,iBAAiC;AAC5E,MAAI,CAAC,OAAO;AAAQ,WAAO;AAC3B,QAAM,SAAS,CAAC,GAAG,MAAM,EAAE,KAAK,CAAC,GAAG,MAAM,IAAI,CAAC;AAC/C,QAAM,QAAQ,KAAK,IAAI,OAAO,SAAS,GAAG,KAAK,MAAO,kBAAkB,MAAO,OAAO,MAAM,CAAC;AAC7F,SAAO,OAAO,KAAK;AACrB;AAEO,SAAS,gBAAgB,QAAkB,GAAmB;AACnE,QAAM,MAAM,KAAK,MAAM;AACvB,QAAM,YAAY,MAAM,MAAM;AAC9B,SAAO,MAAM,IAAI;AACnB;;;ACvBO,SAAS,cAAc,QAAkB,QAA0B;AACxE,MAAI,UAAU;AAAG,WAAO,CAAC,GAAG,MAAM;AAClC,QAAM,SAAmB,CAAC;AAC1B,WAAS,IAAI,GAAG,IAAI,OAAO,QAAQ,KAAK;AACtC,UAAM,QAAQ,KAAK,IAAI,GAAG,IAAI,KAAK,MAAM,SAAS,CAAC,CAAC;AACpD,UAAM,MAAM,KAAK,IAAI,OAAO,QAAQ,IAAI,KAAK,KAAK,SAAS,CAAC,CAAC;AAC7D,UAAM,QAAQ,OAAO,MAAM,OAAO,GAAG;AACrC,UAAM,UAAU,MAAM,OAAO,CAAC,KAAK,UAAU,MAAM,OAAO,CAAC,IAAI,MAAM;AACrE,WAAO,KAAK,OAAO;AAAA,EACrB;AACA,SAAO;AACT;AAEO,SAAS,UAAU,QAA4B;AACpD,MAAI,CAAC,OAAO;AAAQ,WAAO,CAAC;AAC5B,QAAM,MAAM,KAAK,IAAI,GAAG,MAAM;AAC9B,QAAM,MAAM,KAAK,IAAI,GAAG,MAAM;AAC9B,MAAI,QAAQ;AAAK,WAAO,OAAO,IAAI,MAAM,CAAC;AAC1C,SAAO,OAAO,IAAI,CAAC,WAAW,QAAQ,QAAQ,MAAM,IAAI;AAC1D;;;ACVO,SAAS,qBAAqB,MAAc,UAA6B,CAAC,GAAa;AAC5F,QAAM,EAAE,YAAY,GAAG,kBAAkB,GAAG,iBAAiB,IAAI,IAAI;AACrE,QAAM,YAAY,mBAAmB,IAAI;AACzC,MAAI,UAAU,UAAU;AAAG,WAAO,CAAC;AAEnC,QAAM,gBAA0B,CAAC;AACjC,WAAS,IAAI,GAAG,IAAI,UAAU,SAAS,GAAG,KAAK;AAC7C,UAAM,aAAa,SAAS,UAAU,KAAK,IAAI,GAAG,IAAI,YAAY,CAAC,CAAC,EAAE,QAAQ,EAAE,IAAI,CAAC,UAAU,MAAM,MAAM,YAAY,CAAC;AACxH,UAAM,cAAc,SAAS,UAAU,KAAK,IAAI,UAAU,SAAS,GAAG,IAAI,SAAS,CAAC,EAAE,QAAQ,EAAE,IAAI,CAAC,UAAU,MAAM,MAAM,YAAY,CAAC;AACxI,UAAM,UAAU,WAAW,OAAO,CAAC,UAAU,YAAY,SAAS,KAAK,CAAC,EAAE;AAC1E,UAAM,QAAQ,UAAU,KAAK,IAAI,GAAG,KAAK,KAAK,WAAW,SAAS,YAAY,MAAM,CAAC;AACrF,kBAAc,KAAK,KAAK;AAAA,EAC1B;AAEA,QAAM,WAAW,cAAc,eAAe,eAAe;AAC7D,QAAM,aAAa,UAAU,QAAQ;AAErC,QAAM,aAAuB,CAAC;AAC9B,WAAS,IAAI,GAAG,IAAI,WAAW,SAAS,GAAG,KAAK;AAC9C,UAAM,QAAQ,WAAW,IAAI,CAAC,IAAI,WAAW,CAAC,KAAK,WAAW,IAAI,CAAC,IAAI,WAAW,CAAC;AACnF,QAAI,QAAQ,kBAAkB,WAAW,CAAC,IAAI,WAAW,IAAI,CAAC,KAAK,WAAW,CAAC,IAAI,WAAW,IAAI,CAAC,GAAG;AACpG,iBAAW,KAAK,UAAU,CAAC,EAAE,GAAG;AAAA,IAClC;AAAA,EACF;AACA,SAAO;AACT;;;ACzBA,eAAsB,cAAc,MAAc,UAAoB,UAAsB,CAAC,GAAsB;AACjH,QAAM,EAAE,SAAS,EAAE,IAAI;AACvB,QAAM,YAAY,mBAAmB,IAAI;AACzC,MAAI,UAAU,UAAU;AAAG,WAAO,CAAC;AACnC,QAAM,UAAU,MAAM,SAAS,MAAM,UAAU,IAAI,CAAC,MAAM,EAAE,QAAQ,CAAC;AACrE,QAAM,SAAqB,MAAM;AAAA,IAAK,EAAE,QAAQ,QAAQ,OAAO;AAAA,IAAG,CAAC,GAAG,MACpE,QAAQ,IAAI,CAAC,QAAQ,iBAAiB,KAAK,QAAQ,CAAC,CAAC,CAAC;AAAA,EACxD;AAEA,QAAM,aAAuB,CAAC;AAC9B,WAAS,IAAI,QAAQ,IAAI,OAAO,SAAS,QAAQ,KAAK;AACpD,QAAI,eAAe;AACnB,QAAI,gBAAgB;AAEpB,aAAS,IAAI,IAAI,QAAQ,IAAI,GAAG,KAAK;AACnC,sBAAgB,IAAI,OAAO,CAAC,EAAE,CAAC;AAAA,IACjC;AACA,aAAS,IAAI,GAAG,IAAI,IAAI,QAAQ,KAAK;AACnC,uBAAiB,IAAI,OAAO,CAAC,EAAE,CAAC;AAAA,IAClC;AAEA,UAAM,WAAW,KAAK,IAAI,eAAe,aAAa,IAAI;AAC1D,QAAI,WAAW,KAAK;AAClB,iBAAW,KAAK,UAAU,CAAC,EAAE,GAAG;AAAA,IAClC;AAAA,EACF;AAEA,SAAO;AACT;;;AC9BA,SAAS,eAAe,UAAuC;AAC7D,QAAM,SAAS,SAAS,SAAS,YAAY,CAAC;AAC9C,QAAM,OAAO,oBAAI,IAAoB;AACrC,aAAW,SAAS,QAAQ;AAC1B,SAAK,IAAI,MAAM,QAAQ,KAAK,IAAI,MAAM,KAAK,KAAK,KAAK,CAAC;AAAA,EACxD;AACA,SAAO;AACT;AAEA,SAAS,cAAc,WAAqB,OAAe,KAAqB;AAC9E,QAAM,SAAS,oBAAI,IAAoB;AACvC,MAAI,QAAQ;AACZ,WAAS,IAAI,OAAO,IAAI,KAAK,KAAK;AAChC,UAAM,MAAM,eAAe,UAAU,CAAC,CAAC;AACvC,eAAW,CAAC,OAAO,KAAK,KAAK,IAAI,QAAQ,GAAG;AAC1C,aAAO,IAAI,QAAQ,OAAO,IAAI,KAAK,KAAK,KAAK,KAAK;AAClD,eAAS;AAAA,IACX;AAAA,EACF;AACA,MAAI,UAAU;AACd,aAAW,CAAC,EAAE,KAAK,KAAK,OAAO,QAAQ,GAAG;AACxC,UAAM,IAAI,QAAQ;AAClB,eAAW,QAAQ,KAAK,IAAI,CAAC;AAAA,EAC/B;AACA,SAAO;AACT;AAEO,SAAS,mBAAmB,MAAc,UAA2B,CAAC,GAAa;AACxF,QAAM,EAAE,cAAc,GAAG,mBAAmB,EAAE,IAAI;AAClD,QAAM,YAAY,mBAAmB,IAAI;AACzC,MAAI,UAAU,UAAU;AAAG,WAAO,CAAC;AAEnC,QAAM,KAAe,MAAM,UAAU,SAAS,CAAC,EAAE,KAAK,SAAS;AAC/D,QAAM,YAAsB,MAAM,UAAU,SAAS,CAAC,EAAE,KAAK,EAAE;AAC/D,KAAG,CAAC,IAAI;AAER,WAAS,IAAI,kBAAkB,KAAK,UAAU,QAAQ,KAAK;AACzD,aAAS,IAAI,KAAK,IAAI,GAAG,IAAI,EAAE,GAAG,KAAK,IAAI,kBAAkB,KAAK;AAChE,YAAM,gBAAgB,UAAU,CAAC,MAAM,KAAK,IAAI,KAAK;AACrD,UAAI,eAAe;AAAa;AAChC,YAAM,QAAQ,GAAG,CAAC,IAAI,cAAc,UAAU,IAAI,CAAC,MAAM,EAAE,QAAQ,GAAG,GAAG,CAAC;AAC1E,UAAI,QAAQ,GAAG,CAAC,GAAG;AACjB,WAAG,CAAC,IAAI;AACR,kBAAU,CAAC,IAAI;AAAA,MACjB;AAAA,IACF;AAAA,EACF;AAEA,QAAM,aAAuB,CAAC;AAC9B,MAAI,MAAM,UAAU;AACpB,SAAO,MAAM,KAAK,UAAU,GAAG,MAAM,IAAI;AACvC,eAAW,KAAK,UAAU,MAAM,CAAC,EAAE,GAAG;AACtC,UAAM,UAAU,GAAG;AAAA,EACrB;AAEA,SAAO,WAAW,QAAQ;AAC5B;;;ANrDA,IAAM,kBAAN,MAA0C;AAAA,EACxC,MAAM,MAAM,OAAsC;AAChD,UAAM,YAAY;AAClB,WAAO,MAAM,IAAI,CAAC,SAAS;AACzB,YAAM,SAAS,IAAI,MAAM,SAAS,EAAE,KAAK,CAAC;AAC1C,YAAM,SAAS,SAAS,KAAK,YAAY,CAAC;AAC1C,aAAO,QAAQ,CAAC,UAAU;AACxB,cAAM,OAAO,KAAK,KAAK,MAAM,KAAK,IAAI;AACtC,eAAO,IAAI,KAAK;AAAA,MAClB,CAAC;AACD,aAAO;AAAA,IACT,CAAC;AAAA,EACH;AAAA,EAEQ,KAAK,OAAuB;AAClC,QAAI,OAAO;AACX,aAAS,IAAI,GAAG,IAAI,MAAM,QAAQ,KAAK;AACrC,cAAQ,QAAQ,KAAK,OAAO,MAAM,WAAW,CAAC;AAC9C,cAAQ;AAAA,IACV;AACA,WAAO,KAAK,IAAI,IAAI;AAAA,EACtB;AACF;AAEO,IAAM,kBAAN,MAAyC;AAAA,EAS9C,YAAY,SAAyB;AACnC,SAAK,YAAY,QAAQ,aAAa;AACtC,SAAK,WAAW,QAAQ,gBAAgB;AACxC,SAAK,WAAW,QAAQ,YAAY,IAAI,gBAAgB;AACxD,SAAK,UAAU,QAAQ,WAAW;AAClC,SAAK,kBAAkB,QAAQ,mBAAmB;AAClD,SAAK,aAAa,QAAQ,cAAc;AACxC,SAAK,oBAAoB,QAAQ,qBAAqB;AAAA,EACxD;AAAA,EAEA,MAAM,MAAM,MAAgC;AAC1C,UAAM,YAAY,mBAAmB,IAAI;AACzC,QAAI,UAAU,WAAW;AAAG,aAAO,CAAC;AACpC,QAAI,UAAU,WAAW,GAAG;AAC1B,aAAO;AAAA,QACL;AAAA,UACE,IAAIC,MAAK;AAAA,UACT,MAAM,UAAU,CAAC,EAAE;AAAA,UACnB,OAAO,UAAU,CAAC,EAAE;AAAA,UACpB,KAAK,UAAU,CAAC,EAAE;AAAA,QACpB;AAAA,MACF;AAAA,IACF;AAEA,UAAM,aAAa,MAAM,KAAK,SAAS,MAAM,UAAU,IAAI,CAAC,aAAa,SAAS,QAAQ,CAAC;AAC3F,UAAM,eAAe,eAAe,UAAU;AAC9C,UAAM,WAAW,cAAc,cAAc,KAAK,eAAe;AACjE,UAAM,kBAAkB,MAAM,KAAK,kBAAkB,MAAM,UAAU,IAAI,CAAC,MAAM,EAAE,QAAQ,GAAG,QAAQ;AAErG,UAAM,SAAkB,CAAC;AACzB,QAAI,YAAY;AAChB,eAAW,SAAS,iBAAiB;AACnC,YAAM,gBAAgB,UAAU,SAAS;AACzC,YAAM,cAAc,UAAU,KAAK;AACnC,YAAM,YAAY,UAAU,MAAM,WAAW,QAAQ,CAAC,EAAE,IAAI,CAAC,aAAa,SAAS,QAAQ,EAAE,KAAK,GAAG;AACrG,aAAO,KAAK,EAAE,IAAIA,MAAK,GAAG,MAAM,WAAW,OAAO,cAAc,OAAO,KAAK,YAAY,IAAI,CAAC;AAC7F,kBAAY,QAAQ;AAAA,IACtB;AAEA,QAAI,YAAY,UAAU,QAAQ;AAChC,YAAM,gBAAgB,UAAU,SAAS;AACzC,YAAM,cAAc,UAAU,UAAU,SAAS,CAAC;AAClD,YAAM,YAAY,UAAU,MAAM,SAAS,EAAE,IAAI,CAAC,aAAa,SAAS,QAAQ,EAAE,KAAK,GAAG;AAC1F,aAAO,KAAK,EAAE,IAAIA,MAAK,GAAG,MAAM,WAAW,OAAO,cAAc,OAAO,KAAK,YAAY,IAAI,CAAC;AAAA,IAC/F;AAEA,WAAO,KAAK,iBAAiB,MAAM;AAAA,EACrC;AAAA,EAEA,MAAc,kBAAkB,MAAc,WAAqB,cAA2C;AAC5G,YAAQ,KAAK,UAAU;AAAA,MACrB,KAAK;AACH,eAAO,KAAK,iBAAiB,YAAY;AAAA,MAC3C,KAAK;AACH,eAAO,KAAK,qBAAqB,YAAY;AAAA,MAC/C,KAAK;AACH,eAAO,KAAK,sBAAsB,YAAY;AAAA,MAChD,KAAK;AACH,eAAO,KAAK,mBAAmB,YAAY;AAAA,MAC7C,KAAK;AACH,eAAO,KAAK,kBAAkB,MAAM,qBAAqB,IAAI,CAAC;AAAA,MAChE,KAAK;AACH,eAAO,KAAK,kBAAkB,MAAM,MAAM,cAAc,MAAM,KAAK,QAAQ,CAAC;AAAA,MAC9E,KAAK;AACH,eAAO,KAAK,kBAAkB,MAAM,mBAAmB,IAAI,CAAC;AAAA,MAC9D;AACE,eAAO,KAAK,iBAAiB,YAAY;AAAA,IAC7C;AAAA,EACF;AAAA,EAEQ,iBAAiB,cAAkC;AACzD,UAAM,YAAY,gBAAgB,cAAc,KAAK,OAAO;AAC5D,WAAO,aACJ,IAAI,CAAC,OAAO,WAAW,EAAE,OAAO,MAAM,EAAE,EACxC,OAAO,CAAC,EAAE,MAAM,MAAM,QAAQ,SAAS,EACvC,IAAI,CAAC,EAAE,MAAM,MAAM,KAAK;AAAA,EAC7B;AAAA,EAEQ,qBAAqB,cAAkC;AAC7D,UAAM,YAAY,WAAgB,cAAc,KAAK,UAAU;AAC/D,WAAO,aACJ,IAAI,CAAC,OAAO,WAAW,EAAE,OAAO,MAAM,EAAE,EACxC,OAAO,CAAC,EAAE,MAAM,MAAM,SAAS,SAAS,EACxC,IAAI,CAAC,EAAE,MAAM,MAAM,KAAK;AAAA,EAC7B;AAAA,EAEQ,sBAAsB,cAAkC;AAC9D,UAAM,aAAuB,CAAC;AAC9B,aAAS,IAAI,GAAG,IAAI,aAAa,SAAS,GAAG,KAAK;AAChD,UAAI,aAAa,CAAC,IAAI,aAAa,IAAI,CAAC,KAAK,aAAa,CAAC,IAAI,aAAa,IAAI,CAAC,GAAG;AAClF,mBAAW,KAAK,CAAC;AAAA,MACnB;AAAA,IACF;AACA,WAAO;AAAA,EACT;AAAA,EAEQ,mBAAmB,cAAkC;AAC3D,UAAM,aAAuB,CAAC;AAC9B,aAAS,IAAI,GAAG,IAAI,aAAa,SAAS,GAAG,KAAK;AAChD,YAAM,WAAW,aAAa,IAAI,CAAC,IAAI,aAAa,CAAC;AACrD,UAAI,WAAW,CAAC,KAAK,mBAAmB;AACtC,mBAAW,KAAK,CAAC;AAAA,MACnB;AAAA,IACF;AACA,WAAO;AAAA,EACT;AAAA,EAEQ,kBAAkB,MAAc,mBAAuC;AAC7E,UAAM,YAAY,mBAAmB,IAAI;AACzC,UAAM,UAAoB,CAAC;AAC3B,sBAAkB,QAAQ,CAAC,aAAa;AACtC,YAAM,QAAQ,UAAU,UAAU,CAAC,aAAa,SAAS,OAAO,QAAQ;AACxE,UAAI,UAAU,IAAI;AAChB,gBAAQ,KAAK,KAAK;AAAA,MACpB;AAAA,IACF,CAAC;AACD,WAAO;AAAA,EACT;AAAA,EAEQ,iBAAiB,QAA0B;AACjD,UAAM,SAAkB,CAAC;AACzB,QAAI,SAAuB;AAC3B,eAAW,SAAS,QAAQ;AAC1B,UAAI,SAAS,MAAM,IAAI,EAAE,SAAS,KAAK,YAAY,KAAK;AACtD,eAAO,KAAK,GAAG,KAAK,gBAAgB,KAAK,CAAC;AAC1C;AAAA,MACF;AACA,UAAI,CAAC,QAAQ;AACX,iBAAS;AACT;AAAA,MACF;AACA,YAAM,WAAW,GAAG,OAAO,IAAI,IAAI,MAAM,IAAI,GAAG,KAAK;AACrD,UAAI,SAAS,QAAQ,EAAE,UAAU,KAAK,WAAW;AAC/C,iBAAS;AAAA,UACP,IAAI,OAAO;AAAA,UACX,MAAM;AAAA,UACN,OAAO,OAAO;AAAA,UACd,KAAK,MAAM;AAAA,QACb;AAAA,MACF,OAAO;AACL,eAAO,KAAK,MAAM;AAClB,iBAAS;AAAA,MACX;AAAA,IACF;AACA,QAAI,QAAQ;AACV,aAAO,KAAK,MAAM;AAAA,IACpB;AACA,WAAO;AAAA,EACT;AAAA,EAEQ,gBAAgB,OAAuB;AAC7C,UAAM,SAAS,SAAS,MAAM,IAAI;AAClC,UAAM,SAAkB,CAAC;AACzB,aAAS,IAAI,GAAG,IAAI,OAAO,QAAQ,KAAK,KAAK,WAAW;AACtD,YAAM,QAAQ,OAAO,MAAM,GAAG,IAAI,KAAK,SAAS;AAChD,YAAM,OAAO,MAAM,IAAI,CAAC,UAAU,MAAM,KAAK,EAAE,KAAK,GAAG;AACvD,YAAM,cAAc,MAAM,CAAC,EAAE;AAC7B,YAAM,YAAY,MAAM,MAAM,SAAS,CAAC,EAAE;AAC1C,aAAO,KAAK;AAAA,QACV,IAAIA,MAAK;AAAA,QACT;AAAA,QACA,OAAO,MAAM,QAAQ;AAAA,QACrB,KAAK,MAAM,QAAQ;AAAA,MACrB,CAAC;AAAA,IACH;AACA,WAAO;AAAA,EACT;AACF;;;AOlNA,SAAS,MAAMC,aAAY;AAKpB,IAAM,gBAAN,MAAuC;AAAA,EAK5C,YAAY,SAAyB;AACnC,SAAK,WAAW,IAAI,gBAAgB,EAAE,GAAG,SAAS,MAAM,WAAW,CAAC;AACpE,SAAK,eAAe,QAAQ,gBAAgB,KAAK,OAAO,QAAQ,aAAa,OAAO,CAAC;AACrF,SAAK,eAAe,QAAQ,iBAAiB,QAAQ,aAAa,OAAO;AAAA,EAC3E;AAAA,EAEA,MAAM,MAAM,MAAgC;AAC1C,UAAM,aAAa,oBAAoB,IAAI;AAC3C,UAAM,SAAkB,CAAC;AAEzB,eAAW,aAAa,YAAY;AAClC,YAAM,SAAS,SAAS,UAAU,QAAQ;AAC1C,UAAI,OAAO,UAAU,KAAK,cAAc;AACtC,eAAO,KAAK,EAAE,IAAIC,MAAK,GAAG,MAAM,UAAU,UAAU,OAAO,UAAU,OAAO,KAAK,UAAU,IAAI,CAAC;AAAA,MAClG,OAAO;AACL,cAAM,iBAAiB,MAAM,KAAK,SAAS,MAAM,UAAU,QAAQ;AACnE,uBAAe,QAAQ,CAAC,UAAU;AAChC,iBAAO,KAAK;AAAA,YACV,GAAG;AAAA,YACH,OAAO,UAAU,QAAQ,MAAM;AAAA,YAC/B,KAAK,UAAU,QAAQ,MAAM;AAAA,UAC/B,CAAC;AAAA,QACH,CAAC;AAAA,MACH;AAAA,IACF;AAEA,WAAO,KAAK,cAAc,MAAM;AAAA,EAClC;AAAA,EAEQ,cAAc,QAA0B;AAC9C,UAAM,WAAoB,CAAC;AAC3B,QAAI,SAAuB;AAE3B,eAAW,SAAS,QAAQ;AAC1B,UAAI,CAAC,QAAQ;AACX,iBAAS;AACT;AAAA,MACF;AACA,YAAM,WAAW,GAAG,OAAO,IAAI;AAAA,EAAK,MAAM,IAAI,GAAG,KAAK;AACtD,YAAM,iBAAiB,SAAS,QAAQ,EAAE;AAC1C,UAAI,iBAAiB,KAAK,cAAc;AACtC,iBAAS;AAAA,UACP,IAAI,OAAO;AAAA,UACX,MAAM;AAAA,UACN,OAAO,OAAO;AAAA,UACd,KAAK,MAAM;AAAA,QACb;AAAA,MACF,WAAW,iBAAiB,KAAK,cAAc;AAC7C,iBAAS,KAAK,MAAM;AACpB,iBAAS;AAAA,MACX,OAAO;AACL,iBAAS;AAAA,UACP,IAAI,OAAO;AAAA,UACX,MAAM;AAAA,UACN,OAAO,OAAO;AAAA,UACd,KAAK,MAAM;AAAA,QACb;AAAA,MACF;AAAA,IACF;AAEA,QAAI,QAAQ;AACV,eAAS,KAAK,MAAM;AAAA,IACtB;AAEA,WAAO;AAAA,EACT;AACF;;;AC5EA,SAAS,MAAMC,aAAY;AAK3B,IAAM,uBAAN,MAA+C;AAAA,EAC7C,MAAM,MAAM,OAAsC;AAChD,UAAM,YAAY;AAClB,WAAO,MAAM,IAAI,CAAC,SAAS;AACzB,YAAM,SAAS,IAAI,MAAM,SAAS,EAAE,KAAK,CAAC;AAC1C,YAAM,SAAS,SAAS,KAAK,YAAY,CAAC;AAC1C,aAAO,QAAQ,CAAC,UAAU;AACxB,cAAM,OAAO,KAAK,KAAK,MAAM,KAAK,IAAI;AACtC,eAAO,IAAI,KAAK;AAAA,MAClB,CAAC;AACD,aAAO;AAAA,IACT,CAAC;AAAA,EACH;AAAA,EAEQ,KAAK,OAAuB;AAClC,QAAI,OAAO;AACX,aAAS,IAAI,GAAG,IAAI,MAAM,QAAQ,KAAK;AACrC,cAAQ,QAAQ,KAAK,OAAO,MAAM,WAAW,CAAC;AAC9C,cAAQ;AAAA,IACV;AACA,WAAO,KAAK,IAAI,IAAI;AAAA,EACtB;AACF;AAEO,IAAM,eAAN,MAAsC;AAAA,EAI3C,YAAY,SAAyB;AACnC,SAAK,WAAW,QAAQ,YAAY,IAAI,qBAAqB;AAC7D,SAAK,aAAa,QAAQ;AAAA,EAC5B;AAAA,EAEA,MAAM,MAAM,MAAgC;AAC1C,UAAM,YAAY,mBAAmB,IAAI;AACzC,QAAI,CAAC,UAAU;AAAQ,aAAO,CAAC;AAC/B,UAAM,UAAU,MAAM,KAAK,SAAS,MAAM,UAAU,IAAI,CAAC,MAAM,EAAE,QAAQ,CAAC;AAC1E,UAAM,IAAI,KAAK;AAAA,MACb,UAAU;AAAA,MACV,KAAK,cAAc,KAAK,IAAI,GAAG,KAAK,MAAM,KAAK,KAAK,UAAU,MAAM,CAAC,CAAC;AAAA,IACxE;AACA,UAAM,cAAc,KAAK,OAAO,SAAS,CAAC;AAE1C,UAAM,SAAkB,CAAC;AACzB,QAAI,iBAAiB,YAAY,CAAC;AAClC,QAAI,aAAa;AACjB,aAAS,IAAI,GAAG,IAAI,YAAY,QAAQ,KAAK;AAC3C,UAAI,YAAY,CAAC,MAAM,gBAAgB;AACrC,cAAMC,iBAAgB,UAAU,UAAU;AAC1C,cAAMC,eAAc,UAAU,IAAI,CAAC;AACnC,cAAMC,aAAY,UAAU,MAAM,YAAY,CAAC,EAAE,IAAI,CAAC,MAAM,EAAE,QAAQ,EAAE,KAAK,GAAG;AAChF,eAAO,KAAK,EAAE,IAAIC,MAAK,GAAG,MAAMD,YAAW,OAAOF,eAAc,OAAO,KAAKC,aAAY,KAAK,UAAU,EAAE,SAAS,eAAe,EAAE,CAAC;AACpI,qBAAa;AACb,yBAAiB,YAAY,CAAC;AAAA,MAChC;AAAA,IACF;AAEA,UAAM,gBAAgB,UAAU,UAAU;AAC1C,UAAM,cAAc,UAAU,UAAU,SAAS,CAAC;AAClD,UAAM,YAAY,UAAU,MAAM,UAAU,EAAE,IAAI,CAAC,MAAM,EAAE,QAAQ,EAAE,KAAK,GAAG;AAC7E,WAAO,KAAK,EAAE,IAAIE,MAAK,GAAG,MAAM,WAAW,OAAO,cAAc,OAAO,KAAK,YAAY,KAAK,UAAU,EAAE,SAAS,eAAe,EAAE,CAAC;AAEpI,WAAO;AAAA,EACT;AAAA,EAEQ,OAAO,SAAqB,GAAW,aAAa,IAAc;AACxE,UAAM,YAAY,QAAQ,MAAM,GAAG,CAAC,EAAE,IAAI,CAAC,QAAQ,CAAC,GAAG,GAAG,CAAC;AAC3D,UAAM,cAAc,IAAI,MAAM,QAAQ,MAAM,EAAE,KAAK,CAAC;AAEpD,aAAS,YAAY,GAAG,YAAY,YAAY,aAAa;AAC3D,UAAI,UAAU;AACd,eAAS,IAAI,GAAG,IAAI,QAAQ,QAAQ,KAAK;AACvC,YAAI,YAAY;AAChB,YAAI,eAAe;AACnB,iBAAS,IAAI,GAAG,IAAI,GAAG,KAAK;AAC1B,gBAAM,WAAW,KAAK,kBAAkB,QAAQ,CAAC,GAAG,UAAU,CAAC,CAAC;AAChE,cAAI,WAAW,cAAc;AAC3B,2BAAe;AACf,wBAAY;AAAA,UACd;AAAA,QACF;AACA,YAAI,YAAY,CAAC,MAAM,WAAW;AAChC,sBAAY,CAAC,IAAI;AACjB,oBAAU;AAAA,QACZ;AAAA,MACF;AAEA,UAAI,CAAC;AAAS;AAEd,YAAM,OAAO,MAAM,KAAK,EAAE,QAAQ,EAAE,GAAG,MAAM,IAAI,MAAM,QAAQ,CAAC,EAAE,MAAM,EAAE,KAAK,CAAC,CAAC;AACjF,YAAM,SAAS,IAAI,MAAM,CAAC,EAAE,KAAK,CAAC;AAClC,cAAQ,QAAQ,CAAC,QAAQ,UAAU;AACjC,cAAM,UAAU,YAAY,KAAK;AACjC,eAAO,OAAO;AACd,iBAAS,IAAI,GAAG,IAAI,OAAO,QAAQ,KAAK;AACtC,eAAK,OAAO,EAAE,CAAC,KAAK,OAAO,CAAC;AAAA,QAC9B;AAAA,MACF,CAAC;AACD,eAAS,IAAI,GAAG,IAAI,GAAG,KAAK;AAC1B,YAAI,OAAO,CAAC,MAAM;AAAG;AACrB,kBAAU,CAAC,IAAI,KAAK,CAAC,EAAE,IAAI,CAAC,UAAU,QAAQ,OAAO,CAAC,CAAC;AAAA,MACzD;AAAA,IACF;AAEA,WAAO;AAAA,EACT;AAAA,EAEQ,kBAAkB,GAAa,GAAqB;AAC1D,UAAM,SAAS,KAAK,IAAI,EAAE,QAAQ,EAAE,MAAM;AAC1C,QAAI,MAAM;AACV,aAAS,IAAI,GAAG,IAAI,QAAQ,KAAK;AAC/B,YAAM,QAAQ,EAAE,CAAC,KAAK,MAAM,EAAE,CAAC,KAAK;AACpC,aAAO,OAAO;AAAA,IAChB;AACA,WAAO,KAAK,KAAK,GAAG;AAAA,EACtB;AACF;;;ACzHA,SAAS,MAAMC,aAAY;AAIpB,IAAM,uBAAN,MAA8C;AAAA,EAInD,YAAY,SAAyB;AACnC,SAAK,YAAY,QAAQ,aAAa;AACtC,SAAK,UAAU,QAAQ,gBAAgB,KAAK,MAAM,KAAK,YAAY,CAAC;AAAA,EACtE;AAAA,EAEA,MAAM,MAAM,MAAgC;AAC1C,UAAM,YAAY,mBAAmB,IAAI;AACzC,QAAI,CAAC,UAAU;AAAQ,aAAO,CAAC;AAE/B,UAAM,SAAS,SAAS,IAAI;AAC5B,QAAI,CAAC,OAAO;AAAQ,aAAO,CAAC;AAE5B,UAAM,SAAS,KAAK,IAAI,GAAG,KAAK,YAAY,KAAK,OAAO;AACxD,UAAM,SAAkB,CAAC;AAEzB,aAAS,QAAQ,GAAG,QAAQ,OAAO,QAAQ,SAAS,QAAQ;AAC1D,YAAM,MAAM,KAAK,IAAI,OAAO,QAAQ,QAAQ,KAAK,SAAS;AAC1D,YAAM,cAAc,OAAO,MAAM,OAAO,GAAG;AAC3C,YAAM,YAAY,YAAY,IAAI,CAAC,UAAU,MAAM,KAAK,EAAE,KAAK,GAAG,EAAE,KAAK;AACzE,YAAM,aAAa,YAAY,CAAC,EAAE;AAClC,YAAM,WAAW,YAAY,YAAY,SAAS,CAAC,EAAE;AAErD,YAAM,mBAAmB,KAAK,2BAA2B,YAAY,UAAU,SAAS;AACxF,aAAO,KAAK,EAAE,IAAIC,MAAK,GAAG,MAAM,WAAW,OAAO,iBAAiB,OAAO,KAAK,iBAAiB,IAAI,CAAC;AACrG,UAAI,QAAQ,OAAO;AAAQ;AAAA,IAC7B;AAEA,WAAO;AAAA,EACT;AAAA,EAEQ,2BAA2B,OAAe,KAAa,WAAkF;AAC/I,QAAI,gBAAgB;AACpB,QAAI,cAAc;AAClB,eAAW,YAAY,WAAW;AAChC,UAAI,SAAS,SAAS,SAAS,SAAS,OAAO,OAAO;AACpD,wBAAgB,SAAS;AAAA,MAC3B;AACA,UAAI,SAAS,SAAS,OAAO,SAAS,OAAO,KAAK;AAChD,sBAAc,SAAS;AAAA,MACzB;AAAA,IACF;AACA,WAAO,EAAE,OAAO,eAAe,KAAK,YAAY;AAAA,EAClD;AACF;;;AC1CA,IAAM,WAAW,oBAAI,IAAgE;AAErF,SAAS,IAAI,SAAS,CAAC,YAAY,IAAI,aAAa,OAAO,CAAC;AAC5D,SAAS,IAAI,aAAa,CAAC,YAAY,IAAI,iBAAiB,OAAO,CAAC;AACpE,SAAS,IAAI,YAAY,CAAC,YAAY,IAAI,gBAAgB,OAAO,CAAC;AAClE,SAAS,IAAI,UAAU,CAAC,YAAY,IAAI,cAAc,OAAO,CAAC;AAC9D,SAAS,IAAI,SAAS,CAAC,YAAY,IAAI,aAAa,OAAO,CAAC;AAC5D,SAAS,IAAI,WAAW,CAAC,YAAY,IAAI,qBAAqB,OAAO,CAAC;AAE/D,SAAS,gBAAgB,MAAc,SAA+C;AAC3F,WAAS,IAAI,MAAM,OAAO;AAC5B;AAEO,SAAS,cAAc,SAAkC;AAC9D,QAAM,UAAU,SAAS,IAAI,QAAQ,IAAI;AACzC,MAAI,CAAC,SAAS;AACZ,UAAM,IAAI,MAAM,yBAAyB,QAAQ,IAAI,EAAE;AAAA,EACzD;AACA,SAAO,QAAQ,OAAO;AACxB;;;ACpBO,IAAM,iBAAN,MAAyC;AAAA,EAK9C,YAAY,UAAiC,CAAC,GAAG;AAC/C,SAAK,SAAS,QAAQ,UAAU,QAAQ,IAAI,kBAAkB;AAC9D,SAAK,QAAQ,QAAQ,SAAS;AAC9B,SAAK,WAAW,QAAQ,YAAY;AACpC,QAAI,CAAC,KAAK,QAAQ;AAChB,YAAM,IAAI,MAAM,+CAA+C;AAAA,IACjE;AAAA,EACF;AAAA,EAEA,MAAM,MAAM,OAAsC;AAChD,UAAM,WAAW,MAAM,MAAM,KAAK,UAAU;AAAA,MAC1C,QAAQ;AAAA,MACR,SAAS;AAAA,QACP,gBAAgB;AAAA,QAChB,eAAe,UAAU,KAAK,MAAM;AAAA,MACtC;AAAA,MACA,MAAM,KAAK,UAAU,EAAE,OAAO,OAAO,OAAO,KAAK,MAAM,CAAC;AAAA,IAC1D,CAAC;AAED,QAAI,CAAC,SAAS,IAAI;AAChB,YAAM,UAAU,MAAM,SAAS,KAAK;AACpC,YAAM,IAAI,MAAM,oCAAoC,OAAO,EAAE;AAAA,IAC/D;AAEA,UAAM,OAAQ,MAAM,SAAS,KAAK;AAClC,WAAO,KAAK,KAAK,IAAI,CAAC,UAAU,MAAM,SAAS;AAAA,EACjD;AACF;;;AChCO,IAAM,sBAAN,MAA8C;AAAA,EAKnD,YAAY,UAAsC,CAAC,GAAG;AACpD,SAAK,SAAS,QAAQ,UAAU,QAAQ,IAAI;AAC5C,SAAK,QAAQ,QAAQ,SAAS;AAC9B,SAAK,WAAW,QAAQ,YAAY,oEAAoE,KAAK,KAAK;AAAA,EACpH;AAAA,EAEA,MAAM,MAAM,OAAsC;AAChD,UAAM,YAAwB,CAAC;AAC/B,eAAW,QAAQ,OAAO;AACxB,YAAM,WAAW,MAAM,MAAM,KAAK,UAAU;AAAA,QAC1C,QAAQ;AAAA,QACR,SAAS;AAAA,UACP,gBAAgB;AAAA,UAChB,GAAI,KAAK,SAAS,EAAE,eAAe,UAAU,KAAK,MAAM,GAAG,IAAI,CAAC;AAAA,QAClE;AAAA,QACA,MAAM,KAAK,UAAU,EAAE,QAAQ,KAAK,CAAC;AAAA,MACvC,CAAC;AACD,UAAI,CAAC,SAAS,IAAI;AAChB,cAAM,UAAU,MAAM,SAAS,KAAK;AACpC,cAAM,IAAI,MAAM,0CAA0C,OAAO,EAAE;AAAA,MACrE;AACA,YAAM,OAAQ,MAAM,SAAS,KAAK;AAClC,gBAAU,KAAK,MAAM,QAAQ,KAAK,CAAC,CAAC,IAAK,KAAoB,CAAC,IAAK,IAAiB;AAAA,IACtF;AACA,WAAO;AAAA,EACT;AACF;;;ACnCO,IAAM,qBAAN,MAA6C;AAAA,EAClD,YAA6B,SAA6B;AAA7B;AAAA,EAA8B;AAAA,EAE3D,MAAM,MAAM,OAAsC;AAChD,UAAM,SAAS,MAAM,KAAK,QAAQ,KAAK;AACvC,WAAO;AAAA,EACT;AACF;","names":["uuid","uuid","uuid","uuid","uuid","uuid","uuid","startSentence","endSentence","chunkText","uuid","uuid","uuid"]}